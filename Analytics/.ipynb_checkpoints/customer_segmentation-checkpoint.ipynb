{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6f9db-4418-4fc1-8d4e-ded3c4971bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Libraries imported ---\n",
      "--- Step 2: Database connection successful! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juriel\\AppData\\Local\\Temp\\ipykernel_20672\\1693751035.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, db)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Fetched Raw Customer Data ---\n",
      "    CustID  order_frequency  total_spent  average_spend  days_since_last_order\n",
      "0  C25-005                6       4357.5     726.250000                      1\n",
      "1  C25-006                3       2185.0     728.333333                      8\n",
      "2  C25-007                4       2283.0     570.750000                      1\n",
      "3  C25-008                2        872.0     436.000000                      4\n",
      "4  C25-009                2       1384.0     692.000000                      3\n",
      "\n",
      "--- Step 4: Preparing data for clustering ---\n",
      "Data scaled successfully.\n",
      "\n",
      "--- Step 5: Performing K-Means clustering ---\n",
      "Clustering complete.\n",
      "\n",
      "--- Step 6: Cluster Analysis (Averages) ---\n",
      "         order_frequency  total_spent  average_spend  days_since_last_order\n",
      "cluster                                                                    \n",
      "2               6.000000  4357.500000     726.250000               1.000000\n",
      "3               2.333333  1714.666667     735.944444               4.333333\n",
      "0               3.000000  1577.500000     503.375000               2.500000\n",
      "1               1.000000   525.000000     525.000000              33.000000\n",
      "\n",
      "Segment names assigned:\n",
      "    CustID         segment_name\n",
      "0  C25-005  High-Value Spenders\n",
      "1  C25-006       Loyal Regulars\n",
      "2  C25-007    New or Occasional\n",
      "3  C25-008    New or Occasional\n",
      "4  C25-009       Loyal Regulars\n",
      "\n",
      "--- Step 7: Saving results to the database ---\n",
      "'Customer_Segments' table ready.\n",
      "✅ Successfully updated segments for 7 customers in the database.\n"
     ]
    }
   ],
   "source": [
    "# This notebook connects to the LaundroLink database, performs all analyses,\n",
    "# and saves the results into pre-calculated tables for the dashboards.\n",
    "\n",
    "# --- 1. Import Libraries ---\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "print(\"--- Step 1: Libraries imported ---\")\n",
    "\n",
    "# --- 2. Database Connection ---\n",
    "try:\n",
    "    load_dotenv('../Backend/.env') \n",
    "    db = mysql.connector.connect(\n",
    "        host=os.getenv(\"DB_HOST\"),\n",
    "        user=os.getenv(\"DB_USER\"),\n",
    "        password=os.getenv(\"DB_PASSWORD\"),\n",
    "        database=os.getenv(\"DB_NAME\")\n",
    "    )\n",
    "    cursor = db.cursor()\n",
    "    print(\"--- Step 2: Database connection successful! ---\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"--- ❌ Step 2 FAILED: Error connecting to database: {err} ---\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Fetch Customer Behavior Data ---\n",
    "print(\"\\n--- Starting Customer Segmentation Analysis ---\")\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    c.CustID,\n",
    "    COUNT(DISTINCT o.OrderID) AS order_frequency,\n",
    "    SUM(i.PayAmount) AS total_spent,\n",
    "    AVG(i.PayAmount) AS average_spend,\n",
    "    DATEDIFF(NOW(), MAX(o.OrderCreatedAt)) as days_since_last_order\n",
    "FROM Customer c\n",
    "JOIN Orders o ON c.CustID = o.CustID\n",
    "JOIN Invoice i ON o.OrderID = i.OrderID\n",
    "WHERE i.InvoiceID IN (SELECT InvoiceID FROM Invoice_Status WHERE InvoiceStatus = 'Paid')\n",
    "GROUP BY c.CustID\n",
    "HAVING order_frequency > 0;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, db)\n",
    "print(\"\\n--- Step 3: Fetched Raw Customer Data ---\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# --- 4. Data Preparation for Clustering ---\n",
    "print(\"\\n--- Step 4: Preparing data for clustering ---\")\n",
    "\n",
    "features = df[['order_frequency', 'total_spent', 'average_spend', 'days_since_last_order']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "print(\"Data scaled successfully.\")\n",
    "\n",
    "\n",
    "# --- 5. Perform K-Means Clustering ---\n",
    "print(\"\\n--- Step 5: Performing K-Means clustering ---\")\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "df['cluster'] = kmeans.fit_predict(scaled_features)\n",
    "print(\"Clustering complete.\")\n",
    "\n",
    "\n",
    "# --- 6. Analyze and Name the Segments ---\n",
    "cluster_analysis = df.groupby('cluster')[['order_frequency', 'total_spent', 'average_spend', 'days_since_last_order']].mean().sort_values('total_spent', ascending=False)\n",
    "print(\"\\n--- Step 6: Cluster Analysis (Averages) ---\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "cluster_map = {}\n",
    "sorted_clusters = cluster_analysis.reset_index()\n",
    "\n",
    "cluster_map[sorted_clusters.loc[0, 'cluster']] = 'High-Value Spenders'\n",
    "cluster_map[sorted_clusters.loc[1, 'cluster']] = 'Loyal Regulars'\n",
    "\n",
    "at_risk_cluster = sorted_clusters.sort_values('days_since_last_order', ascending=False).iloc[0]['cluster']\n",
    "if at_risk_cluster not in cluster_map:\n",
    "    cluster_map[at_risk_cluster] = 'At-Risk Customers'\n",
    "\n",
    "for i in range(4):\n",
    "    if i not in cluster_map:\n",
    "        cluster_map[i] = 'New or Occasional'\n",
    "\n",
    "df['segment_name'] = df['cluster'].map(cluster_map)\n",
    "print(\"\\nSegment names assigned.\")\n",
    "\n",
    "\n",
    "# --- 7. Save Customer Segment Results to Database ---\n",
    "print(\"\\n--- Step 7: Saving customer segments to database ---\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Customer_Segments (\n",
    "    CustID VARCHAR(10) PRIMARY KEY,\n",
    "    SegmentName VARCHAR(50),\n",
    "    AnalyzedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "    FOREIGN KEY (CustID) REFERENCES Customer(CustID) ON DELETE CASCADE\n",
    ");\n",
    "\"\"\")\n",
    "print(\"'Customer_Segments' table ready.\")\n",
    "\n",
    "# Clear old data before inserting new analysis\n",
    "cursor.execute(\"TRUNCATE TABLE Customer_Segments;\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    cursor.execute(\"INSERT INTO Customer_Segments (CustID, SegmentName) VALUES (%s, %s)\", (row['CustID'], row['segment_name']))\n",
    "\n",
    "db.commit()\n",
    "print(f\"✅ Customer Segmentation Complete. Updated {cursor.rowcount} customers.\")\n",
    "\n",
    "\n",
    "# --- 8. Analyze Popular Services for Each Shop ---\n",
    "print(\"\\n--- Starting Popular Services Analysis ---\")\n",
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Shop_Popular_Services (\n",
    "        ShopID VARCHAR(10),\n",
    "        SvcName VARCHAR(20),\n",
    "        orderCount INT,\n",
    "        AnalyzedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "        PRIMARY KEY (ShopID, SvcName),\n",
    "        FOREIGN KEY (ShopID) REFERENCES Laundry_Shop(ShopID) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\")\n",
    "    print(\"'Shop_Popular_Services' table ready.\")\n",
    "\n",
    "    cursor.execute(\"TRUNCATE TABLE Shop_Popular_Services;\")\n",
    "\n",
    "    query_services = \"\"\"\n",
    "        INSERT INTO Shop_Popular_Services (ShopID, SvcName, orderCount)\n",
    "        SELECT ShopID, SvcName, orderCount FROM (\n",
    "            SELECT \n",
    "                o.ShopID,\n",
    "                s.SvcName,\n",
    "                COUNT(o.OrderID) as orderCount,\n",
    "                ROW_NUMBER() OVER(PARTITION BY o.ShopID ORDER BY COUNT(o.OrderID) DESC) as rn\n",
    "            FROM Orders o\n",
    "            JOIN Service s ON o.SvcID = s.SvcID\n",
    "            GROUP BY o.ShopID, s.SvcName\n",
    "        ) AS RankedServices\n",
    "        WHERE rn <= 5;\n",
    "    \"\"\"\n",
    "    cursor.execute(query_services)\n",
    "    db.commit()\n",
    "    print(f\"✅ Successfully updated popular services for {cursor.rowcount} shop/service combinations.\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"--- ❌ Step 8 FAILED: {err} ---\")\n",
    "\n",
    "\n",
    "# --- 9. Analyze Busiest Times for Each Shop ---\n",
    "print(\"\\n--- Starting Busiest Times Analysis ---\")\n",
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Shop_Busiest_Times (\n",
    "        ShopID VARCHAR(10),\n",
    "        timeSlot VARCHAR(50),\n",
    "        orderCount INT,\n",
    "        AnalyzedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "        PRIMARY KEY (ShopID, timeSlot),\n",
    "        FOREIGN KEY (ShopID) REFERENCES Laundry_Shop(ShopID) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\")\n",
    "    print(\"'Shop_Busiest_Times' table ready.\")\n",
    "    \n",
    "    cursor.execute(\"TRUNCATE TABLE Shop_Busiest_Times;\")\n",
    "\n",
    "    query_times = \"\"\"\n",
    "        INSERT INTO Shop_Busiest_Times (ShopID, timeSlot, orderCount)\n",
    "        SELECT\n",
    "            ShopID,\n",
    "            CASE\n",
    "                WHEN HOUR(OrderCreatedAt) BETWEEN 7 AND 11 THEN 'Morning (7am-12pm)'\n",
    "                WHEN HOUR(OrderCreatedAt) BETWEEN 12 AND 16 THEN 'Afternoon (12pm-5pm)'\n",
    "                ELSE 'Evening (5pm onwards)'\n",
    "            END as timeSlot,\n",
    "            COUNT(OrderID) as orderCount\n",
    "        FROM Orders\n",
    "        GROUP BY ShopID, timeSlot;\n",
    "    \"\"\"\n",
    "    cursor.execute(query_times)\n",
    "    db.commit()\n",
    "    print(f\"✅ Successfully updated busiest times for {cursor.rowcount} shop/time combinations.\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"--- ❌ Step 9 FAILED: {err} ---\")\n",
    "\n",
    "\n",
    "# --- 10. Final Cleanup ---\n",
    "cursor.close()\n",
    "db.close()\n",
    "print(\"\\n--- All analyses complete. Database connection closed. ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442f6d3-0686-454d-bb2a-646acb2c7a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
